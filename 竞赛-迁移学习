迁移学习：
      数据集同分布的
      Domain：不要求数据分布一致，希望减少数据分布差异，解决源域和目标域的分布差异的问题
      task：
      分类：归纳式迁移学习-目标任务不同但相关
           直推式迁移学习-目标任务相同，目标数据域没有可获得的带标记的数据，源数据域中有可获得的带标记的数据
           无监督：目标域和源域都没有标签，关注目标任务的聚类，降维和密度估计
      分类： 同构的DA：数据空间一致，数据分布不一致。监督，半监督，无监督
            异构的DA：数据空间不一致，一个图片，一个文本
      Domain adaptation ： 共享特征
为什么fine tune 有效？  1.预训练提取了浅层基础特征和深层抽象特征 2.有效避免从头开始训练，不需要大量数据，节省计算时间和计算资源3.模型不收敛，参数不优化，模型泛化能力差的问题
      
半监督：先训练有标签的数据，然后给没有标签的数据打标。这样没有标签的数据有了伪标签，然后将打好标签的数据用小权重和有标签的数据的大权重放在一起训练，
       最小化loss
             1.固定W1和W2的值，真实标签的个数为n,未标注样本个数为m,W1对应真实样本权重，W2对应未标注样本权重
                经验值： m/n 在 [5*W1/W2,10*W1/W2]
             2.探索一下线上和线下的标签分布，根据标签分布进行W权重的调整(扰动）
             
       Deep Domain Confusion Maximizing for Domain Invariance
