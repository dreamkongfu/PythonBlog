分析错分类，是不是特征没有抽取到
原因:  1.BadCase 样例过少 2.数据标注错误
问题： 能否BadCase数据单独建模

思路：
1.BadCase样例数据增强    概率较低，预测错误较多，属于错误样本
2.去除可能的标记错误样本

Stacking：深度学习，数据量比较大的情况

集成学习: Bagging(森林），Boosting（AdaBoost,GBDT,XGBoost,LightGBM,CatBoost),Stacking,Blending
目的：减少方差， 同一个模型的预测值的方差    找分数比较接近的 ，多次平均可以减少方差，10折效果好于5折。过拟合的方差比较大
     减少偏差： 预测值avg和真实值avg的差的平方
     两个模型相关性不大，则融合的效果更好。尽量都是分数高的
Bagging(森林）：降低方差，1.特征采样和数据采样，2.不同特征训练多个模型（bagging） 3.投票的方法，平均，sigma，softMax(逆向回Y，然后相加）
               抽样的方法：有放回的采样，等，统计量（booststrap，偏差，方差，BadCase
               评价： 5折获取不够把数据的80% ，可以提高到90%，可以10折，适合小数据集
Boosting：容易过拟合，预测值-label, 然后再做一个模型，残差提升
         AdaBoost(面试）预测正确的样本权重降低，预测错误的样本权重上升，调整样本的权重，多个样本融合

Stacking: 容易过拟合， 把CNN特征拿出来，LSTM特征拿出来 ，一般方法可以提高
          一般不用概率来做Stacking，容易过拟合
          把预测值当作特征，当作第二层样本的特征 + 第二个模型
          
          
Blending： 数据集分为两份，一份预测，一份测试，训练两次就好
           会把预测值当作特征
半监督 会把 预测值当作训练样本集， 有效，小数据集
尝试尽可能多的解题思路
