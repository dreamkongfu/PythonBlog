1.为什么神经网络中加入非线性是必须的。
f1 = w*X+b 是线性函数，如果第二层网络 f2 = w*X+b 的话，f2(f1(x) = w*(w*X+b)+b 仍然是线性的函数关系。
对非线性函数不能很好得拟合
加入非线性函数，z = 1/(1+e^(-x) , z(f1(x) 对非线性函数就是很好得模拟
2.激活函数的表达式，及其导数。绘制原函数及其导数的图像，总结各激活函数的利弊。
sigmoid =  1/(1+e^(-x)        delSigmoid = 1/(e^x+2+e^(-x))
tanH = (1-e^(-x))/(1+e^(-x))  delTanH = 2/(e^x+2+e^(-x))
relu = x  x >0
       0  x =0 
sigmoid 和 tanH 优点是比较稳定，全程可导， 输出区间；用于 RNN 缺点是 在饱和区梯度容易消失，不利于加深网络结构
relu 的优点是梯度恒为1，更容易学习优化，解决深层神经网络的收敛问题  缺点是不稳定，输出值容易发散，不能用于RNN
3.分类与回归的区别
回归 是连续的，MSE
分类 是离散的 ， Cross Entropy
softMax 用于非线性变换，计算Yi的概率分布

sk = e^yk /(∑e^yi)
4.CE 手推
Kuler-back lelbler 两个概率分布差异  KL(p(x)|q(x)) = ∑P(xi)logP(xi)/Q(xi)
标注S‘ （000001000） 输出S
KL(S’｜S） = ∑S‘ilog(S'i/Si) = 1log(1/Sk) = -log(Sk)= log(∑e^yi)-yk
delKL/delyi = e^yi/∑e^yi-ó(yk) = S - S'
.
批量梯度下降 全部的样本都计算
    优点： 全局最优解，可以并行运算
    缺点：比较慢，占内存容量
随机梯度下降， 每次只算一个样本
    优点： 比较快，不占内存
    缺点： 不一定得到全局最优解或者收敛，不可以并行运算
小批量梯度下降 加载小批量
    优点：比较快，可以并行运算，可以收敛
    缺点： Batch size 的选择

