optimize:
    1.RNN_forward； return loss, cache(values of layers),da
    2.RNN_backward: input da, cache, xt,return gradients
    3.clip
    4.update_gradients:input: learning rate,gradients, return gradients
    return: loss,da_prev
model:
     1. optimize under interation 
     return results.
     
keras.model
     Layers:LSTM,Dense,Activation,Dropout,Input,Masking
     callbacks: LambdaCallback
     1.buld model: Input-x->LSTM_cell-a->Dense->output ->Model 
     2.model compile: optimizer=Adam(learning rate,beta1,beta2), loss,metrics
     3.model.fit: Input,Output,epochs
 layer 层学习
 input: 参数： shape[值(n,m,p)]，batch_size(值:n),sparse(值[true,falese])，ragged（值[true,false],处理non-uniform shape,variable length list, nest object）
        返回值：placeHolder tensor
        instance ： x = Input(shape=(n,m)) // placeholder tensor
  Dense: 参数：units(值：n,output dimension),activation(值:['relu','softmax'],use_bias(值:[true,false],是否使用bias vector),kernel_initializer,bias_initializer,kernel_regularizer,bias_regularizer,activity_regularizer,kernel_constraint,bias_constraint
         返回值: placeHolder tensor
         instance: y = Dense(16,activation='softmax')(x)
  Activation: 参数：activation(值:[function , string])
         返回值: placeHolder tensor
         instance: Activation(activation='relu')
  Lambda: 参数: function,output_shape,mask,arguments
          返回值: placeHolder tensor
          instance: lamb = Lambda(lambda x : x **2)(x)
  Embedding:参数: input_dim,output_dim,embeddings_initializer,embeddings_regularizer,activity_regularizer,embeddings_constraint,mask_zero,input_length
          返回值: placeHolder tensor, shape=(batch_size,input_length,output_dim)
  Droupout: 参数： rate(值:[0,1]),noise_shape,seed,
           返回值: placeHolder tensor
           instance: d = Dropout(0.2,input_shape=(2,))
           call arguments: inputs, training(值:[true,false])
  LSTM 参数:units(值：n,dimension of output),activation(值:['tanh']),recurrent_activation(值:['relu','sigmoid',用于recurrent_step]),use_bias(值:[true,false]),
           kernel_initializer,recurrent_initializer,bias_initializer,unit_forget_bias,
           kernel_regularizer,recurrent_regularizer,bias_regularizer,activity_regularizer,
           kernel_constraint,recurrent_constraint,bias_constraint,
           dropout(值:[0-1],用于input transformation),recurrent_dropout(值:[0,1],用于recurrent State的变化),implementation(值:[1,2],1代表bigger number of smaller opertions,2代表fewer number of larger options,默认2),
           return_sequences(值:[true,false],是否返回一系列output, 还是返回最后一个结果，默认false),return_state(值:[true,false],是否返回最后的状态,默认false),go_backwards(值:[true,false(default)],是否逆序处理input),
           stateful(值:[true,false(default)],last state of each sample of i in batch 是否用于下一个state),time_major(值:[true,false(default)],如果为true，input,output shape 为[time_steps,batch,features],效率更高一些，否则Batch_major 模式input,output 为[batch,time_steps,features]),
           unroll(值:[true,false(default)],if true, network will be unrolled, more efficient but heavery in memory , 适合短句子；否则，loop will be used
       返回值; placeHolder tensor
       instance； LSTM_cell = LSTM(n,return_state=true)
       调用参数：inputs(值:tensor ,shape=[batch,timesteps,features]),mask(binary tensor,shape=[batch,timesteps,features],用来表示指定的timeStep是否被masked),
               training((值:[true,false(default)],是否为training mode,只针对dropout and recurrent_dropout),initial_state(list of initial state)
           
 Model；参数：inputs(值:keras.Input object or list of keras.Input objects),outputs:(值:functional api),name
        内部函数：summary 参数(line_length(值:n,打印行的长度),position(值=[0-1,0-1,0-1,0-1],位置),print_fn(值:custom funtion, called each line summary)
                         返回； 参数个数，各层级
                         
                get_layer 参数(name(值:name of layer), index(值:index of layer))
                keras.utils.plot_model(model, "my_first_model.png"，show_shapes(值:[true,false])) 参数可以 打印出来model的结构
                
        training API       
                compile 参数(optimizer(值:['adm','rmsprop'] or optimizer instance),loss(值:['mse','categorical_crossentropy'],或者function(y_true,y_pred)或者Loss Instance)),metrics(值:['accuracy','mse'],accuracy可以根据损失函数和input判断是binary_accuray,categorical_accuracy,SparseCategorical_accuracy),
                            loss_weights(值:list of scalar coefficients,衡量不同模型outputs 的损失权重),weighted_metrics(值:评估度量值，加权以sample_weight或者class_weight),run_eagerly(值:[true,false(default)],如果是true,则model logic不被包含在tf函数里)
                fit 参数(x(值:numpy array 或者tensor或者tf.data或者generator or tf.utils.sequence,input data),y(值:numpy.array或者tensor或者tf.data或者generator,tf.utils.sequence),batch_size,epochs(值:n),verbose(值:[0,1,2],为0的时候表示slient,为1的时候表示progess bar,为2的时候表示one line per epoch),
                         callbacks(值:list of callbacks,应用到training上),validation_split(值:[0-1],is not supported by sequence,generator,dataset),
                        validation_data,shuffle（值:[true,false,'batch'],在每一个epoch之前打乱训练数据，'batch' for HDF5 file in batch-sized chunks,not alonth with steps_per_epoch  ）,class_weight(),sample_weight,initial_epoch(值:n,epoch to start training),
                        steps_per_epoch,validation_steps,validation_batch_size,validation_freq,
                        max_queue_size,workers,use_multiprocessing)
                     返回值:a History object, History.history 纪录 训练损失，validation 损失 和相应的epoch
                evaluate
                predict
                train_on_batch
                test_on_batch
                predict_on_batch
                         
